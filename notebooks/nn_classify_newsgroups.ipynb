{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn: 0.21.3\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "print('sklearn:', sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "X = dataset.data\n",
    "y = OneHotEncoder(categories = 'auto').fit_transform(dataset.target.reshape(-1, 1))\n",
    "\n",
    "number_of_targets = len(set(dataset.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk: 3.4.1\n",
      "re: 2.2.1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('nltk:', nltk.__version__)\n",
    "print('re:', re.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(dirty):\n",
    "    clean_version = []\n",
    "    for text in dirty:\n",
    "        text = re.sub(r'[\\s,]+', ' ', text)\n",
    "        text = re.sub(r'[-]+', '', text)\n",
    "        text = re.sub(r'(\\w)(\\/)(\\s)', r'\\1 \\2\\3', text)\n",
    "        \n",
    "        post_tokens = []\n",
    "        \n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            for token in word_tokenize(sentence):\n",
    "                post_tokens.append(token)\n",
    "                \n",
    "        text = ' '.join(post_tokens)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        clean_version.append(text)\n",
    "    \n",
    "    return clean_version\n",
    "        \n",
    "X = clean_text(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## max_df = remove words that appear too frequently,\n",
    "##          ie: .50 -> remove words that appear in more than 50% of documents\n",
    "##          ie: 50 -> remove words that appear in more than 50 documents\n",
    "\n",
    "## min_df = remove words that appear too infrequently,\n",
    "##          ie: .50 -> remove words that appear in less than 50% of the documents\n",
    "##          ie: 50 -> remove words that appear in less than 50 documents\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "vectorizer = CountVectorizer(\n",
    "    binary = True,\n",
    "    stop_words = stop_words,\n",
    "    lowercase = True,\n",
    "    min_df = 5,\n",
    "    max_df = 0.80,\n",
    "    max_features = 8000\n",
    ")\n",
    "\n",
    "x_train_onehot = vectorizer.fit_transform(X_train)\n",
    "number_of_features = len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.5\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1105 07:41:53.734017 4532311488 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W1105 07:41:53.747394 4532311488 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1105 07:41:53.750072 4532311488 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1105 07:41:53.751450 4532311488 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1105 07:41:53.753255 4532311488 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1105 07:41:53.839478 4532311488 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 200)               1600200   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                4020      \n",
      "=================================================================\n",
      "Total params: 1,604,220\n",
      "Trainable params: 1,604,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "nn = Sequential()\n",
    "\n",
    "nn.add(Dense(units = 200, activation = 'relu', input_dim = number_of_features))\n",
    "nn.add(Dense(units = number_of_targets, activation = 'softmax'))\n",
    "\n",
    "nn.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1105 07:41:53.954663 4532311488 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7919/7919 [==============================] - 2s 297us/step - loss: 2.1668 - acc: 0.5333\n",
      "Epoch 2/5\n",
      "7919/7919 [==============================] - 2s 267us/step - loss: 0.9651 - acc: 0.8217\n",
      "Epoch 3/5\n",
      "7919/7919 [==============================] - 2s 273us/step - loss: 0.5843 - acc: 0.8980\n",
      "Epoch 4/5\n",
      "7919/7919 [==============================] - 2s 272us/step - loss: 0.4022 - acc: 0.9295\n",
      "Epoch 5/5\n",
      "7919/7919 [==============================] - 2s 269us/step - loss: 0.3039 - acc: 0.9486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122adc780>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(x_train_onehot, y_train, epochs = 5, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3395/3395 [==============================] - 0s 74us/step\n",
      "accuracy: 0.7228276877761414\n"
     ]
    }
   ],
   "source": [
    "scores = nn.evaluate(vectorizer.transform(X_test), y_test)\n",
    "\n",
    "print('accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
